name: AWM Complete Website Scraper

on:
  workflow_dispatch:
  schedule:
    # Run monthly on the 1st at 2 AM UTC
    - cron: '0 2 1 * *'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 pandas openpyxl lxml
    
    - name: Create output directory
      run: mkdir -p output
    
    - name: Run comprehensive scraper
      run: python awm_scraper.py
    
    - name: Compress large output
      run: |
        cd output
        tar -czf ../awm_complete_data.tar.gz *
        cd ..
    
    - name: Upload Excel file
      uses: actions/upload-artifact@v4
      with:
        name: awm-excel-data-${{ github.run_number }}
        path: output/awm_scraped_data.xlsx
        retention-days: 90
    
    - name: Upload compressed complete data
      uses: actions/upload-artifact@v4
      with:
        name: awm-complete-data-${{ github.run_number }}
        path: awm_complete_data.tar.gz
        retention-days: 90
    
    - name: Display final summary
      run: |
        echo "Comprehensive scraping completed!"
        echo "Files in output directory:"
        ls -la output/
        if [ -f output/awm_scraped_data.xlsx ]; then
          echo "Excel file size: $(du -h output/awm_scraped_data.xlsx | cut -f1)"
        fi
        echo "Total output directory size: $(du -sh output/ | cut -f1)"
        echo "Compressed archive size: $(du -h awm_complete_data.tar.gz | cut -f1)"
