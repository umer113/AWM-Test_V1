name: AWM Website Scraper

on:
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Maximum number of pages to scrape'
        required: false
        default: '50'
        type: string
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 pandas openpyxl lxml
    
    - name: Create output directory
      run: mkdir -p output
    
    - name: Run scraper
      env:
        MAX_PAGES: ${{ github.event.inputs.max_pages || '50' }}
      run: python awm_scraper.py
    
    - name: Upload Excel artifact
      uses: actions/upload-artifact@v4
      with:
        name: awm-scraped-data-${{ github.run_number }}
        path: output/awm_scraped_data.xlsx
        retention-days: 30
    
    - name: Upload all output files
      uses: actions/upload-artifact@v4
      with:
        name: awm-complete-output-${{ github.run_number }}
        path: output/
        retention-days: 30
    
    - name: Display summary
      run: |
        echo "Scraping completed successfully!"
        echo "Files in output directory:"
        ls -la output/
        if [ -f output/awm_scraped_data.xlsx ]; then
          echo "Excel file size: $(du -h output/awm_scraped_data.xlsx | cut -f1)"
        fi
